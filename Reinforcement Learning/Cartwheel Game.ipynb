{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Divij\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for t in range(500):\n",
    "    random_action = env.action_space.sample()\n",
    "    env.step(random_action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode : 0/20 High Score : 20\n",
      "Game Episode : 1/20 High Score : 21\n"
     ]
    }
   ],
   "source": [
    "for e in range(2):\n",
    "    observation = env.reset()\n",
    "    for t in range(50):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, other_info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Game Episode : {}/{} High Score : {}\".format(e,20,t))\n",
    "            break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95 #Discount Factor\n",
    "        self.epsilon = 1.0 # Exploration Rate: How much to act randomly, \n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.001 \n",
    "        self.model = self._create_model()\n",
    "        \n",
    "        \n",
    "    def _create_model(self):\n",
    "        #Neural Network To Approximate Q-Value function\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24,input_dim=self.state_size,activation='relu')) #1st Hidden Layer\n",
    "        model.add(Dense(24,activation='relu')) #2nd Hidden Layer\n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done)) #remembering previous experiences\n",
    "        \n",
    "    def act(self,state):\n",
    "        # Exploration vs Exploitation\n",
    "        if np.random.rand()<=self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state) # predict reward value based upon current state\n",
    "        return np.argmax(act_values[0]) #Left or Right\n",
    "    \n",
    "    def train(self,batch_size=32): #method that trains NN with experiences sampled from memory\n",
    "        minibatch = random.sample(self.memory,batch_size)\n",
    "        for state,action,reward,next_state,done in minibatch:\n",
    "            \n",
    "            if not done: #boolean \n",
    "                target = reward + self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target = reward\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state,target_f,epochs=1,verbose=0) #single epoch, x =state, y = target_f, loss--> target_f - \n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "output_dir = \"Cartpole_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "batch_size = 32\n",
    "agent = Agent(state_size,action_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode : 0/1000, High Score : 43, Exploration Rate : 0.95\n",
      "Game Episode : 1/1000, High Score : 8, Exploration Rate : 0.91\n",
      "Game Episode : 2/1000, High Score : 22, Exploration Rate : 0.81\n",
      "Game Episode : 3/1000, High Score : 12, Exploration Rate : 0.77\n",
      "Game Episode : 4/1000, High Score : 13, Exploration Rate : 0.72\n",
      "Game Episode : 5/1000, High Score : 12, Exploration Rate : 0.68\n",
      "Game Episode : 6/1000, High Score : 26, Exploration Rate : 0.59\n",
      "Game Episode : 7/1000, High Score : 10, Exploration Rate : 0.56\n",
      "Game Episode : 8/1000, High Score : 11, Exploration Rate : 0.53\n",
      "Game Episode : 9/1000, High Score : 11, Exploration Rate : 0.51\n",
      "Game Episode : 10/1000, High Score : 9, Exploration Rate : 0.48\n",
      "Game Episode : 11/1000, High Score : 11, Exploration Rate : 0.46\n",
      "Game Episode : 12/1000, High Score : 14, Exploration Rate : 0.43\n",
      "Game Episode : 13/1000, High Score : 16, Exploration Rate : 0.39\n",
      "Game Episode : 14/1000, High Score : 9, Exploration Rate : 0.38\n",
      "Game Episode : 15/1000, High Score : 9, Exploration Rate : 0.36\n",
      "Game Episode : 16/1000, High Score : 10, Exploration Rate : 0.34\n",
      "Game Episode : 17/1000, High Score : 10, Exploration Rate : 0.33\n",
      "Game Episode : 18/1000, High Score : 9, Exploration Rate : 0.31\n",
      "Game Episode : 19/1000, High Score : 13, Exploration Rate : 0.29\n",
      "Game Episode : 20/1000, High Score : 38, Exploration Rate : 0.24\n",
      "Game Episode : 21/1000, High Score : 15, Exploration Rate : 0.22\n",
      "Game Episode : 22/1000, High Score : 19, Exploration Rate : 0.2\n",
      "Game Episode : 23/1000, High Score : 47, Exploration Rate : 0.16\n",
      "Game Episode : 24/1000, High Score : 43, Exploration Rate : 0.13\n",
      "Game Episode : 25/1000, High Score : 38, Exploration Rate : 0.11\n",
      "Game Episode : 26/1000, High Score : 67, Exploration Rate : 0.076\n",
      "Game Episode : 27/1000, High Score : 46, Exploration Rate : 0.061\n",
      "Game Episode : 28/1000, High Score : 48, Exploration Rate : 0.048\n",
      "Game Episode : 29/1000, High Score : 72, Exploration Rate : 0.033\n",
      "Game Episode : 30/1000, High Score : 67, Exploration Rate : 0.024\n",
      "Game Episode : 31/1000, High Score : 142, Exploration Rate : 0.012\n",
      "Game Episode : 32/1000, High Score : 115, Exploration Rate : 0.01\n",
      "Game Episode : 33/1000, High Score : 145, Exploration Rate : 0.01\n",
      "Game Episode : 34/1000, High Score : 119, Exploration Rate : 0.01\n",
      "Game Episode : 35/1000, High Score : 144, Exploration Rate : 0.01\n",
      "Game Episode : 36/1000, High Score : 113, Exploration Rate : 0.01\n",
      "Game Episode : 37/1000, High Score : 182, Exploration Rate : 0.01\n",
      "Game Episode : 38/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 39/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 40/1000, High Score : 187, Exploration Rate : 0.01\n",
      "Game Episode : 41/1000, High Score : 177, Exploration Rate : 0.01\n",
      "Game Episode : 42/1000, High Score : 163, Exploration Rate : 0.01\n",
      "Game Episode : 43/1000, High Score : 134, Exploration Rate : 0.01\n",
      "Game Episode : 44/1000, High Score : 171, Exploration Rate : 0.01\n",
      "Game Episode : 45/1000, High Score : 148, Exploration Rate : 0.01\n",
      "Game Episode : 46/1000, High Score : 177, Exploration Rate : 0.01\n",
      "Game Episode : 47/1000, High Score : 139, Exploration Rate : 0.01\n",
      "Game Episode : 48/1000, High Score : 119, Exploration Rate : 0.01\n",
      "Game Episode : 49/1000, High Score : 99, Exploration Rate : 0.01\n",
      "Game Episode : 50/1000, High Score : 117, Exploration Rate : 0.01\n",
      "Game Episode : 51/1000, High Score : 32, Exploration Rate : 0.01\n",
      "Game Episode : 52/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 53/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 54/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 55/1000, High Score : 10, Exploration Rate : 0.01\n",
      "Game Episode : 56/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 57/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 58/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 59/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 60/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 61/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 62/1000, High Score : 10, Exploration Rate : 0.01\n",
      "Game Episode : 63/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 64/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 65/1000, High Score : 10, Exploration Rate : 0.01\n",
      "Game Episode : 66/1000, High Score : 7, Exploration Rate : 0.01\n",
      "Game Episode : 67/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 68/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 69/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 70/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 71/1000, High Score : 16, Exploration Rate : 0.01\n",
      "Game Episode : 72/1000, High Score : 66, Exploration Rate : 0.01\n",
      "Game Episode : 73/1000, High Score : 7, Exploration Rate : 0.01\n",
      "Game Episode : 74/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 75/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 76/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 77/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 78/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 79/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 80/1000, High Score : 11, Exploration Rate : 0.01\n",
      "Game Episode : 81/1000, High Score : 19, Exploration Rate : 0.01\n",
      "Game Episode : 82/1000, High Score : 117, Exploration Rate : 0.01\n",
      "Game Episode : 83/1000, High Score : 54, Exploration Rate : 0.01\n",
      "Game Episode : 84/1000, High Score : 33, Exploration Rate : 0.01\n",
      "Game Episode : 85/1000, High Score : 51, Exploration Rate : 0.01\n",
      "Game Episode : 86/1000, High Score : 31, Exploration Rate : 0.01\n",
      "Game Episode : 87/1000, High Score : 31, Exploration Rate : 0.01\n",
      "Game Episode : 88/1000, High Score : 61, Exploration Rate : 0.01\n",
      "Game Episode : 89/1000, High Score : 127, Exploration Rate : 0.01\n",
      "Game Episode : 90/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 91/1000, High Score : 192, Exploration Rate : 0.01\n",
      "Game Episode : 92/1000, High Score : 94, Exploration Rate : 0.01\n",
      "Game Episode : 93/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 94/1000, High Score : 177, Exploration Rate : 0.01\n",
      "Game Episode : 95/1000, High Score : 137, Exploration Rate : 0.01\n",
      "Game Episode : 96/1000, High Score : 156, Exploration Rate : 0.01\n",
      "Game Episode : 97/1000, High Score : 125, Exploration Rate : 0.01\n",
      "Game Episode : 98/1000, High Score : 132, Exploration Rate : 0.01\n",
      "Game Episode : 99/1000, High Score : 130, Exploration Rate : 0.01\n",
      "Game Episode : 100/1000, High Score : 44, Exploration Rate : 0.01\n",
      "Game Episode : 101/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 102/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 103/1000, High Score : 181, Exploration Rate : 0.01\n",
      "Game Episode : 104/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 105/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 106/1000, High Score : 162, Exploration Rate : 0.01\n",
      "Game Episode : 107/1000, High Score : 72, Exploration Rate : 0.01\n",
      "Game Episode : 108/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 109/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 110/1000, High Score : 183, Exploration Rate : 0.01\n",
      "Game Episode : 111/1000, High Score : 185, Exploration Rate : 0.01\n",
      "Game Episode : 112/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 113/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 114/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 115/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 116/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 117/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 118/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 119/1000, High Score : 103, Exploration Rate : 0.01\n",
      "Game Episode : 120/1000, High Score : 32, Exploration Rate : 0.01\n",
      "Game Episode : 121/1000, High Score : 10, Exploration Rate : 0.01\n",
      "Game Episode : 122/1000, High Score : 177, Exploration Rate : 0.01\n",
      "Game Episode : 123/1000, High Score : 72, Exploration Rate : 0.01\n",
      "Game Episode : 124/1000, High Score : 142, Exploration Rate : 0.01\n",
      "Game Episode : 125/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 126/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 127/1000, High Score : 166, Exploration Rate : 0.01\n",
      "Game Episode : 128/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 129/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 130/1000, High Score : 183, Exploration Rate : 0.01\n",
      "Game Episode : 131/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 132/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 133/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 134/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 135/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 136/1000, High Score : 151, Exploration Rate : 0.01\n",
      "Game Episode : 137/1000, High Score : 76, Exploration Rate : 0.01\n",
      "Game Episode : 138/1000, High Score : 25, Exploration Rate : 0.01\n",
      "Game Episode : 139/1000, High Score : 12, Exploration Rate : 0.01\n",
      "Game Episode : 140/1000, High Score : 142, Exploration Rate : 0.01\n",
      "Game Episode : 141/1000, High Score : 110, Exploration Rate : 0.01\n",
      "Game Episode : 142/1000, High Score : 154, Exploration Rate : 0.01\n",
      "Game Episode : 143/1000, High Score : 149, Exploration Rate : 0.01\n",
      "Game Episode : 144/1000, High Score : 129, Exploration Rate : 0.01\n",
      "Game Episode : 145/1000, High Score : 109, Exploration Rate : 0.01\n",
      "Game Episode : 146/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 147/1000, High Score : 7, Exploration Rate : 0.01\n",
      "Game Episode : 148/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 149/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 150/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 151/1000, High Score : 7, Exploration Rate : 0.01\n",
      "Game Episode : 152/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 153/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 154/1000, High Score : 10, Exploration Rate : 0.01\n",
      "Game Episode : 155/1000, High Score : 113, Exploration Rate : 0.01\n",
      "Game Episode : 156/1000, High Score : 142, Exploration Rate : 0.01\n",
      "Game Episode : 157/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 158/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 159/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 160/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 161/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 162/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 163/1000, High Score : 130, Exploration Rate : 0.01\n",
      "Game Episode : 164/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 165/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 166/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 167/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 168/1000, High Score : 160, Exploration Rate : 0.01\n",
      "Game Episode : 169/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 170/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 171/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 172/1000, High Score : 10, Exploration Rate : 0.01\n",
      "Game Episode : 173/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 174/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 175/1000, High Score : 7, Exploration Rate : 0.01\n",
      "Game Episode : 176/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 177/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 178/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 179/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 180/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 181/1000, High Score : 7, Exploration Rate : 0.01\n",
      "Game Episode : 182/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 183/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 184/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 185/1000, High Score : 7, Exploration Rate : 0.01\n",
      "Game Episode : 186/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 187/1000, High Score : 10, Exploration Rate : 0.01\n",
      "Game Episode : 188/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 189/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 190/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 191/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 192/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 193/1000, High Score : 8, Exploration Rate : 0.01\n",
      "Game Episode : 194/1000, High Score : 11, Exploration Rate : 0.01\n",
      "Game Episode : 195/1000, High Score : 29, Exploration Rate : 0.01\n",
      "Game Episode : 196/1000, High Score : 41, Exploration Rate : 0.01\n",
      "Game Episode : 197/1000, High Score : 39, Exploration Rate : 0.01\n",
      "Game Episode : 198/1000, High Score : 52, Exploration Rate : 0.01\n",
      "Game Episode : 199/1000, High Score : 48, Exploration Rate : 0.01\n",
      "Game Episode : 200/1000, High Score : 54, Exploration Rate : 0.01\n",
      "Game Episode : 201/1000, High Score : 82, Exploration Rate : 0.01\n",
      "Game Episode : 202/1000, High Score : 143, Exploration Rate : 0.01\n",
      "Game Episode : 203/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 204/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 205/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 206/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 207/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 208/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 209/1000, High Score : 185, Exploration Rate : 0.01\n",
      "Game Episode : 210/1000, High Score : 175, Exploration Rate : 0.01\n",
      "Game Episode : 211/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 212/1000, High Score : 23, Exploration Rate : 0.01\n",
      "Game Episode : 213/1000, High Score : 9, Exploration Rate : 0.01\n",
      "Game Episode : 214/1000, High Score : 180, Exploration Rate : 0.01\n",
      "Game Episode : 215/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 216/1000, High Score : 137, Exploration Rate : 0.01\n",
      "Game Episode : 217/1000, High Score : 122, Exploration Rate : 0.01\n",
      "Game Episode : 218/1000, High Score : 199, Exploration Rate : 0.01\n",
      "Game Episode : 219/1000, High Score : 199, Exploration Rate : 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-373e6cfbf1da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-2182f58b9207>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mtarget_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_f\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#single epoch, x =state, y = target_f, loss--> target_f -\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    871\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 873\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3217\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[0;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m--> 558\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[0;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[1;32m--> 415\u001b[1;33m             ctx=ctx)\n\u001b[0m\u001b[0;32m    416\u001b[0m       \u001b[1;31m# Replace empty list with None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size,action_size) #initialize Agent\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1,state_size])\n",
    "    \n",
    "    for time in range(5000):\n",
    "        env.render()\n",
    "        action = agent.act(state) #action is 0 or 1\n",
    "        next_state, reward, done, other_info = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1,state_size])\n",
    "        agent.remember(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(\"Game Episode : {}/{}, High Score : {}, Exploration Rate : {:.2}\".format(e, n_episodes, time, agent.epsilon))\n",
    "            break\n",
    "        \n",
    "        if len(agent.memory)>batch_size:\n",
    "            agent.train(batch_size)\n",
    "    \n",
    "        if e%50==0:\n",
    "            agent.save(output_dir+\"weights_\"+'{:04d}'.format(e)+\".hdf5\")\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
